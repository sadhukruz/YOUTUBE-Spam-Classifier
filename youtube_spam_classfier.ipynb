{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOUTUBE comments spam Classifier overview\n",
    "\n",
    "This whole project is about classifying as SPAM or NOTSPAM of the comments made for the top trending videos in youtube. \n",
    "\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection#\n",
    "\n",
    "This dataset consists of five CSV files along with other files.\n",
    "\n",
    "This corpus has been collected using the YouTube Data API v3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "Defending SPAM is one of the major goal of the tech giants.Youtube takes major measures to do so. This is the project 11 in the TERM two of my nanodegree program and it provides me an \n",
    "opportunity to use machine learning techniques to solve a problem with my skils gained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "\n",
    "The Main objective of the problem is to classify a corpus of youtube comments collected on five videos overtime and use machine learning\n",
    "techniques to classify them as SPAM or NOTSPAM comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary modules \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the csv files from the loacl machine\n",
    "\n",
    "A = pd.read_csv('Youtube01-Psy.csv')\n",
    "B = pd.read_csv('Youtube02-KatyPerry.csv')\n",
    "C = pd.read_csv('Youtube03-LMFAO.csv')\n",
    "D = pd.read_csv('Youtube04-Eminem.csv')\n",
    "E = pd.read_csv('Youtube05-Shakira.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    COMMENT_ID            AUTHOR  \\\n",
      "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
      "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
      "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
      "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
      "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
      "\n",
      "                  DATE                                            CONTENT  \\\n",
      "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
      "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
      "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
      "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
      "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
      "\n",
      "   CLASS  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n",
      "                                COMMENT_ID               AUTHOR  \\\n",
      "345    z12sjp3zgtqnvlysj23zuxxaolrvd1oj504          Kacy Cluley   \n",
      "346  z132enrpoy35yxpoe04cjr4zur3jvbyq3xo0k    Kasia Fabisiewicz   \n",
      "347      z132jbmxfqm4fjysg23nwjfb2mv2vxnua  Decio Alves Martins   \n",
      "348    z12cdlswetvnejcri04cex0jfwy2u3tzj54         Rafi Hossain   \n",
      "349  z120e5uautvcuper304ccf4bjrjugdpbwrc0k           moaz adnan   \n",
      "\n",
      "                    DATE                                            CONTENT  \\\n",
      "345  2015-06-05T18:59:52  This song means so much to me thank you  soooo...   \n",
      "346  2015-06-05T19:02:05                                             &lt;3﻿   \n",
      "347  2015-06-05T19:29:20  KATY PERRY, I AM THE \"DÉCIO CABELO\", \"DECIO HA...   \n",
      "348  2015-06-05T19:55:08  Honestly speaking except taylor swift and adel...   \n",
      "349  2015-06-05T20:01:23  who is going to reach the billion first : katy...   \n",
      "\n",
      "     CLASS  \n",
      "345      0  \n",
      "346      0  \n",
      "347      1  \n",
      "348      0  \n",
      "349      0  \n"
     ]
    }
   ],
   "source": [
    "#checking if the data is imported properly\n",
    "\n",
    "print(A.head(5))\n",
    "print(B.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are multiple datasets in the given problem, first we need to combine all the datasets to be able to work on them.\n",
    "\n",
    "In python we can combine with the \"concate\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    COMMENT_ID            AUTHOR  \\\n",
      "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
      "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
      "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
      "\n",
      "                  DATE                                            CONTENT  \\\n",
      "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
      "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
      "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
      "\n",
      "   CLASS  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n"
     ]
    }
   ],
   "source": [
    "#combining all the datasets into a dataframe \n",
    "train_df = pd.concat([A,B,C,D,E])\n",
    "print(train_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1956"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the length of the df\n",
    "\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are cecking the total number of comments for the df which are both spam and notspam\n",
    "\n",
    "len(train_df.query('CLASS == 1'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "951"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.query('CLASS == 0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # observations \n",
    " \n",
    "    We see that there are around 2000 comments in Toal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step involves dropping the unnecessary columns from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cln_df = train_df.drop(['COMMENT_ID','AUTHOR','DATE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             CONTENT  CLASS\n",
       "0  Huh, anyway check out this you[tube] channel: ...      1\n",
       "1  Hey guys check out my new channel and our firs...      1\n",
       "2             just for test I have to say murdev.com      1\n",
       "3   me shaking my sexy ass on my channel enjoy ^_^ ﻿      1\n",
       "4            watch?v=vtaRGgvGtWQ   Check this out .﻿      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cln_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1956 entries, 0 to 369\n",
      "Data columns (total 2 columns):\n",
      "CONTENT    1956 non-null object\n",
      "CLASS      1956 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 45.8+ KB\n"
     ]
    }
   ],
   "source": [
    "cln_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1956,)\n",
      "(1956,)\n"
     ]
    }
   ],
   "source": [
    "# defining features and labels for the dataset\n",
    "\n",
    "X = cln_df.CONTENT\n",
    "y = cln_df.CLASS\n",
    "\n",
    "#checking the shape of the dataset\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e57c973a58>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we shall see some visualization for the better idea of the comments\n",
    "\n",
    "# Plotting the number of class counts\n",
    "sns.countplot(x='CLASS',data=cln_df,palette='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Observation\n",
    " \n",
    "From the above histogram,we infer that there are more number of comments of the class 1 than the number of comments in the class 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Now we shall split the data into training and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1467,)\n",
      "(489,)\n",
      "(1467,)\n",
      "(489,)\n"
     ]
    }
   ],
   "source": [
    "#checking the split \n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performing fit and transform simultaneously\n",
    "\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit_transform(X_train)\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "x_train_counts = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1467, 3535)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Frequency\n",
    "\n",
    "\n",
    "In orer to compute the frequency of the words we are using the method TF and TF-IDF\n",
    "\n",
    "TF stands for Term Frequency \n",
    "\n",
    "TF-IDF stands for Term Frequency Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1467, 3535)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tranformer = TfidfTransformer()\n",
    "x_train_tfidf = tranformer.fit_transform(x_train_counts)\n",
    "x_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<489x3535 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2981 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_counts = count_vect.transform(X_test)\n",
    "x_test_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<489x3535 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2981 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_tfidf = tranformer.transform(x_test_counts)\n",
    "x_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this case we are using the Multinomial Naive Bayes model\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[203,  43],\n",
       "       [ 12, 231]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88       246\n",
      "           1       0.84      0.95      0.89       243\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       489\n",
      "   macro avg       0.89      0.89      0.89       489\n",
      "weighted avg       0.89      0.89      0.89       489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadhu\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting and applying the model \n",
    "#logistic Regression in this case \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions = model.predict(x_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model predction is done and now we shall check the accuracy score for the above model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[234,  12],\n",
       "       [ 15, 228]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,new_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95       246\n",
      "           1       0.95      0.94      0.94       243\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       489\n",
      "   macro avg       0.94      0.94      0.94       489\n",
      "weighted avg       0.94      0.94      0.94       489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,new_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations \n",
    "\n",
    "\n",
    "After running both the models it is observed that LogisticRegression model has better F1-Score when compared to the Multinomial Naive Bayes model.\n",
    "\n",
    "In this case we shall select LogisticRegression for more accrute predections.\n",
    "\n",
    "\n",
    "# About logistic regression\n",
    "\n",
    "In this provlem we used the logistic regression to improve the model predections because, Logstic Regression does better predections if there are no outliers.It also performs better when there is no high correlation amoung the predictors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Used \n",
    "\n",
    "Here we are using the F1-score as the metric to measure the performance. The more the F1 score the better the model is performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving the model into pickle file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naivebayesML = open(\"YtbSpam_model.pkl\",\"wb\")\n",
    "randomforest_mo = open(\"YtbSpam_model.pkl\",\"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model,randomforest_mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest_mo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytb_model = open(\"YtbSpam_model.pkl\",\"rb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = pickle.load(ytb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
